# 도메인 특화 LLM과 Domain RAG MVP 정리

## 1. 도메인 특화 LLM이란

도메인 특화는 범용 LLM을 특정 산업, 업무, 주제에 맞게 더 정확하고 실무적으로 활용하도록 최적화하는 것을 의미한다.

- 범용 LLM: 여러 주제에 두루 응답 가능
- 도메인 특화 LLM: 의료, 법률, 금융, 제조, 교육 등 특정 분야에서 더 강한 정확도와 문맥 이해 제공

쉽게 말하면, **똑똑한 일반인**을 **특정 분야의 실무형 전문가처럼 활용**하는 접근이다.

### 왜 필요한가

범용 LLM은 다음 한계가 있다.

- 전문 용어 해석이 부정확할 수 있음
- 업계 문맥 이해가 부족할 수 있음
- 내부 규정, 절차, 문서 형식을 반영하지 못할 수 있음
- 답변은 그럴듯하지만 실무 정확도가 부족할 수 있음

그래서 도메인 특화를 하면 다음 효과를 기대할 수 있다.

- 전문 용어 이해 향상
- 문맥 정확도 향상
- 실무형 답변 강화
- 업무 자동화 적합성 향상

---

## 2. 도메인 특화의 대표 방법

### 2.1 프롬프트 특화

역할, 답변 형식, 금지 규칙, 용어집 등을 프롬프트에 반영하는 방식이다.

- 가장 빠르고 비용이 적음
- 초기에 적용하기 쉬움
- 모델 자체를 바꾸지 않아도 됨

예시:

- "당신은 의료 상담 보조 AI다."
- "제공된 문서 근거로만 답변하라."

### 2.2 RAG

RAG(Retrieval-Augmented Generation)는 관련 문서를 검색한 뒤 그 내용을 근거로 답변하는 방식이다.

- 사내 문서, 매뉴얼, 정책, FAQ 연동에 적합
- 최신 정보 반영이 쉬움
- 모델 재학습 없이도 실무 적용 가능
- 근거 기반 답변에 유리

### 2.3 파인튜닝

특정 데이터로 모델을 추가 학습해 응답 스타일, 특정 작업 성능, 출력 형식을 맞추는 방식이다.

- 응답 스타일 통일에 유리
- 분류, 추출, 고정 양식 출력에 강함
- 비용과 시간, 데이터 품질 관리가 중요함

---

## 3. 도메인 특화와 RAG, 파인튜닝의 관계

- 도메인 특화: 목표
- RAG / 파인튜닝: 그 목표를 달성하는 방법

즉, **도메인 특화는 큰 개념**이고, **RAG와 파인튜닝은 구현 수단**이다.

### 실무 권장 순서

대부분의 실무 환경에서는 다음 순서가 가장 현실적이다.

1. 프롬프트 설계
2. RAG 구축
3. 부족한 부분만 파인튜닝 추가

이유:

- 빠르게 시작 가능
- 최신 문서 반영이 쉬움
- 유지보수 비용이 낮음
- 실패 리스크가 적음

---

## 4. 도메인별 설계 포인트

### 4.1 의료

핵심은 정확성, 근거, 책임 경계다.

- RAG 중심 설계
- 진료 가이드라인, 판독 기준, 약물 정보, 병원 매뉴얼 연동
- 확정 진단처럼 말하지 않도록 통제
- 근거 문서 출처 표시
- "확인 필요", "전문의 확인 필요" 같은 안전장치 필요

### 4.2 법률

핵심은 조문, 판례, 최신 개정 여부다.

- 법령, 시행령, 계약서 템플릿, 사내 법무 가이드 연동
- 조항 근거 제시
- 법률 자문처럼 단정하지 않기
- 최신 개정 반영을 위해 RAG가 중요

### 4.3 제조

핵심은 설비, 공정, 품질, 유지보수다.

- 문서 + 구조화 데이터 연계가 중요
- 설비 매뉴얼, SOP, 점검 이력, 불량 리포트, 센서 로그 활용
- 장애 대응, 품질 이슈 분석, 작업 절차 안내에 적합

### 4.4 금융

핵심은 규제, 컴플라이언스, 설명 책임이다.

- 상품 설명서, 약관, 심사 기준, 컴플라이언스 문서 연동
- 금지 표현 통제 필요 (예: 수익 보장)
- 약관 비교, 상담 보조, 민원 대응 초안에 활용 가능

### 4.5 교육

핵심은 학습 수준별 설명과 개인화다.

- 교안, 강의 자료, 문제은행, 커리큘럼, 학습자 데이터 연동
- 난이도별 설명, 시험 문제 생성, 첨삭 피드백에 적합
- 설명 스타일 통일용 파인튜닝도 잘 맞음

---

## 5. 도메인 특화 LLM 구축 체크리스트

### 5.1 목표 정의

- 어떤 도메인인지
- 어떤 업무를 자동화할 것인지
- 누가 사용할 것인지
- 성공 기준은 무엇인지

예시:

- 사내 매뉴얼 기반 헬프데스크 챗봇
- 의료 상담 기록 요약 보조
- 설비 장애 대응 가이드 챗봇

### 5.2 데이터 준비

- 매뉴얼
- FAQ
- 정책 문서
- 보고서
- 상담 로그
- 표준 서식
- 용어집

정리 작업:

- 중복 제거
- 오래된 문서 제거
- 민감 정보 마스킹
- 오탈자 정리
- 문서별 메타데이터 저장

### 5.3 데이터 구조화

- PDF, DOCX, TXT 등 파싱
- 섹션 분리
- 문단 정리
- 청킹(chunking)
- 문서명, 버전, 권한, 부서 등 메타데이터 부여

### 5.4 RAG 구축

- 임베딩 생성
- 벡터 DB 저장
- 사용자 질문 임베딩
- 관련 문서 검색
- 검색 결과를 LLM 프롬프트에 삽입

### 5.5 프롬프트 설계

- 역할 정의
- 답변 범위
- 금지 규칙
- 모르면 모른다고 답하기
- 출력 형식 통일

### 5.6 평가 체계

- 정확성
- 관련성
- 완전성
- 출처 일치성
- 응답 시간

실무에서는 "좋아 보인다"가 아니라 **측정 가능해야 운영 가능**하다.

### 5.7 안전장치

- 금칙어 필터
- 민감 정보 마스킹
- 권한 없는 문서 차단
- 위험 응답 제한

### 5.8 운영 및 지속 개선

- 질문/응답 로그 저장
- 실패 케이스 분석
- 문서 업데이트 반영
- 프롬프트 개선
- 재평가 및 재배포

---

## 6. 도메인 특화 LLM 아키텍처 개요

기본 흐름은 아래와 같다.

1. 사용자 질문 입력
2. 권한 확인
3. 관련 문서 검색
4. 검색 결과 재정렬
5. 프롬프트 조립
6. LLM 응답 생성
7. 출력 정책 및 안전장치 적용
8. 로그 저장 및 품질 개선

핵심은 **LLM 단독이 아니라 검색, 정책, 운영 체계를 함께 붙인 시스템**이라는 점이다.

---

## 7. AWS형과 On-prem형 차이

### 7.1 AWS형

주요 구성:

- S3: 문서 저장
- Textract / Lambda / Glue: 문서 추출 및 ETL
- OpenSearch / Aurora pgvector: 검색 및 벡터 저장
- Bedrock 또는 외부 LLM API: 생성 모델
- CloudWatch / X-Ray: 운영 모니터링

장점:

- 빠른 구축
- 높은 확장성
- 관리형 서비스 활용 가능

주의점:

- 비용 관리 필요
- 데이터 반출 정책 검토 필요

### 7.2 On-prem형

주요 구성:

- NAS / 파일서버: 문서 저장
- Milvus / Qdrant / Weaviate / pgvector: 벡터 DB
- Elasticsearch / OpenSearch: 키워드 검색
- vLLM / TGI / Ollama: 사내 LLM 서빙
- Prometheus / Grafana / ELK: 운영 모니터링

장점:

- 민감정보 통제 강화
- 사내 보안 정책에 적합

주의점:

- GPU 서버 운영 부담
- 모델 배포와 성능 튜닝 직접 관리 필요

### 7.3 실무 추천

- 빠른 PoC: AWS 또는 하이브리드
- 민감정보 중심: On-prem
- 현실적인 기업 환경: 하이브리드(문서는 사내, 모델은 선택형)

---

## 8. 추천 스택

가장 현실적인 기본 추천 조합은 다음과 같다.

- Frontend: 바닐라 JS 또는 React
- API 서버: FastAPI
- 문서 저장소: S3 또는 NAS
- 문서 처리: Python ETL
- 벡터 DB: Qdrant
- 임베딩: sentence-transformers / BGE 계열
- LLM 서빙: vLLM 또는 OpenAI 호환 API
- 인증: JWT / SSO
- 운영: Prometheus + Grafana + 로그 수집

### 한 줄 추천

**FastAPI + Qdrant + vLLM + PostgreSQL + Redis + S3/NAS**

이 조합은 다음 균형이 좋다.

- 빠르게 시작 가능
- 사내형 / 클라우드형 모두 확장 가능
- 문서 기반 RAG에 적합
- 추후 관리자 기능 확장 가능

---

## 9. Domain RAG MVP 구성 요약

실행 가능한 MVP 기준 핵심 기능은 다음과 같다.

- TXT / PDF 업로드
- 문서 파싱
- 텍스트 청킹
- 실제 임베딩 생성
- Qdrant 저장
- 질문 입력 시 유사 문서 검색
- vLLM(OpenAI 호환)으로 답변 생성
- PostgreSQL에 질의 로그 저장

즉, 다음 흐름이다.

**문서 업로드 -> 인덱싱 -> 질문 -> 관련 문서 검색 -> LLM 답변**

### 주요 구성요소

- FastAPI: API 진입점
- Qdrant: 벡터 검색
- PostgreSQL: 로그 저장
- Redis: 캐시 또는 확장용
- sentence-transformers: 임베딩
- pypdf: PDF 파싱
- httpx: vLLM API 호출

---

## 10. 업그레이드 MVP의 주요 모듈

### 10.1 File Parser

- TXT 파일 읽기
- PDF 텍스트 추출
- 기본 텍스트 정제

### 10.2 Chunker

- 문단 단위 분리
- 길이 기준 청킹
- 오버랩 적용

### 10.3 Embedding Service

- sentence-transformers 기반 임베딩 생성
- 추후 BGE 모델로 교체 가능

### 10.4 Vector Store

- Qdrant 컬렉션 자동 생성
- 청크 벡터 업서트
- 유사도 검색

### 10.5 LLM Service

- vLLM OpenAI 호환 엔드포인트 호출
- 검색된 문맥만 기반으로 답변하도록 프롬프트 구성
- 실패 시 검색 결과 요약으로 대체 가능

### 10.6 RAG Service

- 업로드 파일 저장
- 파일 파싱
- 청킹
- 임베딩 생성
- Qdrant 적재
- 질문 응답 전체 오케스트레이션

---

## 11. 관리자 UI 확장본

기존 백엔드에 테스트용 관리자 화면을 추가한 확장본도 구성할 수 있다.

### 포함 기능

- 상태 확인
- 텍스트 직접 등록
- TXT / PDF 업로드
- 질문 입력
- 답변 표시
- 참고 청크 표시

### 활용 목적

- 개발자 테스트
- 관리자용 운영 검증
- API 호출 없이 브라우저 기반 점검

초기 단계에서는 **정적 HTML + Tailwind 기반 UI**만으로도 충분히 운영 점검이 가능하다.

---

## 12. 다음 단계 확장 포인트

실서비스에 가깝게 만들려면 다음 기능을 순차적으로 붙이면 된다.

### 우선순위 높은 개선

1. 실제 임베딩 모델 최적화
2. 토큰 기반 정교한 청킹
3. 문서 메타데이터 저장
4. 권한 기반 검색 필터
5. 대화 이력 기반 멀티턴 처리
6. 재정렬(reranker) 추가
7. 관리자 인증 및 로그인
8. 문서별 버전 관리

### 운영 관점 강화

- 문서 업로드자 추적
- 인덱싱 상태 관리
- 사용량 로그
- 실패 응답 분석
- 평가셋 기반 품질 점검

---

## 13. 핵심 요약

### 개념

도메인 특화 LLM은 특정 산업과 업무에 맞춰 LLM을 실무적으로 활용하는 방식이다.

### 방법

- 프롬프트 특화
- RAG
- 파인튜닝

### 실무 권장

- 먼저 RAG
- 필요 시 파인튜닝
- 항상 안전장치와 운영 체계 포함

### 추천 스택

**FastAPI + Qdrant + vLLM + PostgreSQL + Redis + S3/NAS**

### MVP 핵심 흐름

**업로드 -> 청킹 -> 임베딩 -> 벡터 검색 -> LLM 답변**

---

## 14. 실무용 한 줄 결론

가장 현실적인 시작점은 다음이다.

**문서 중심 RAG MVP를 먼저 구축하고, 운영 로그를 기반으로 권한, 품질, UI, 재정렬, 파인튜닝을 점진 확장한다.**

